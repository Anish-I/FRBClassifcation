{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23b25cd4-5f1b-468f-b673-9669de602eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.datasets import make_classification\n",
    "import time\n",
    "import GPUtil\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73066e31-c170-4181-8be3-e445777e9cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single run training time: 32.65 seconds\n",
      "Total estimated training time: 793.32 minutes\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "data_frb = pd.read_csv('chimefrbcat1.csv')\n",
    "data_tns = pd.read_csv('tns_search.csv')\n",
    "\n",
    "# Preprocessing FRB data\n",
    "data_frb.dropna(inplace=True)\n",
    "label_encoder = LabelEncoder()\n",
    "data_frb['frb_name_encoded'] = label_encoder.fit_transform(data_frb['tns_name'])\n",
    "features_frb = data_frb[['ra', 'dec', 'bonsai_dm', 'bonsai_snr', 'flux', 'fluence']]\n",
    "labels_frb = data_frb['frb_name_encoded']\n",
    "\n",
    "# Preprocessing TNS data\n",
    "data_tns['dm'] = data_tns['DM']\n",
    "data_tns['flux'] = np.random.uniform(0, 1, len(data_tns))  # Assuming random flux values as they are not provided\n",
    "data_tns['fluence'] = np.random.uniform(0, 1, len(data_tns))  # Assuming random fluence values as they are not provided\n",
    "\n",
    "# Convert RA and DEC from string to float values (simplified approach)\n",
    "data_tns['RA'] = data_tns['RA'].apply(lambda x: sum([float(i) / 60 ** n for n, i in enumerate(x.split(':'))]))\n",
    "data_tns['DEC'] = data_tns['DEC'].apply(lambda x: sum([float(i) / 60 ** n for n, i in enumerate(x.split(':'))]))\n",
    "\n",
    "features_tns = data_tns[['RA', 'DEC', 'dm', 'DM-Err', 'flux', 'fluence']]\n",
    "labels_tns = np.random.randint(0, len(np.unique(labels_frb)), len(data_tns))  # Random labels for simplicity\n",
    "\n",
    "# Combine the datasets\n",
    "features = pd.concat([features_frb, features_tns])\n",
    "labels = np.hstack((labels_frb, labels_tns))\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "features = imputer.fit_transform(features)\n",
    "\n",
    "# Handling class imbalance using resampling\n",
    "majority_class = np.bincount(labels).argmax()\n",
    "minority_classes = np.unique(labels[labels != majority_class])\n",
    "\n",
    "# Upsample minority classes\n",
    "majority_data = features[labels == majority_class]\n",
    "resampled_data = majority_data.copy()\n",
    "\n",
    "for cls in minority_classes:\n",
    "    cls_data = features[labels == cls]\n",
    "    cls_upsampled = resample(cls_data, \n",
    "                             replace=True,\n",
    "                             n_samples=len(majority_data),\n",
    "                             random_state=42)\n",
    "    resampled_data = np.vstack([resampled_data, cls_upsampled])\n",
    "\n",
    "features_resampled = resampled_data\n",
    "labels_resampled = np.hstack((labels[labels == majority_class], np.repeat(minority_classes, len(majority_data))))\n",
    "\n",
    "# Feature engineering\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "features_poly = poly.fit_transform(features_resampled)\n",
    "\n",
    "# Create synthetic data\n",
    "X_synthetic, y_synthetic = make_classification(n_samples=10000, n_features=features_poly.shape[1], \n",
    "                                               n_informative=10, n_redundant=0, random_state=42)\n",
    "\n",
    "X_combined = np.vstack((features_poly, X_synthetic))\n",
    "y_combined = np.hstack((labels_resampled, y_synthetic))\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Benchmark a single run\n",
    "xgb_model_single = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, tree_method='hist', device='cuda', use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "start_time = time.time()\n",
    "xgb_model_single.fit(X_train_scaled, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "single_run_time = end_time - start_time\n",
    "print(f\"Single run training time: {single_run_time:.2f} seconds\")\n",
    "\n",
    "# Calculate total estimated training time\n",
    "total_runs = 486 * 3  # Total grid search combinations * cross-validation folds\n",
    "total_estimated_time = single_run_time * total_runs\n",
    "print(f\"Total estimated training time: {total_estimated_time / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61eabc97-5f44-4ee1-baa7-d60a723eb237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ivatu\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ivatu\\anaconda3\\envs\\py310\\lib\\site-packages\\xgboost\\core.py:160: UserWarning: [01:52:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized GBDT Model - Test accuracy: 0.9100\n"
     ]
    }
   ],
   "source": [
    "# Reduced parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5],\n",
    "    'min_child_weight': [1, 3],\n",
    "    'gamma': [0, 0.1],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8]\n",
    "}\n",
    "\n",
    "# Initialize the model with GPU support\n",
    "xgb_model = XGBClassifier(tree_method='hist', device='cuda', use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters and model\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the model\n",
    "xgb_accuracy = best_xgb_model.score(X_test_scaled, y_test)\n",
    "print(f\"Optimized GBDT Model - Test accuracy: {xgb_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
